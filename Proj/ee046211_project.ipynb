{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzV9wsJ5pGhf"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> EE 046211 - Technion - Deep Learning\n",
        "---\n",
        "\n",
        "## Course Project - Actually good trash statistics\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZZybn3NpGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
        "---\n",
        "\n",
        "|Name     |Campus Email| ID  |\n",
        "|---------|--------------------------------|----------|\n",
        "|Alexander Balabanov| alexander.b@campus.technion.ac.il| 312775364|\n",
        "|Dr. John Zoidberg| (\\\\\\/)(;,,;)(\\\\\\/)@campus.technion.ac.il| 3000|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9uxrDgJ9u_q"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Get Datasets\n",
        "---\n",
        "The project will include 2 dataset that we are going to work with:\n",
        "- [COCO](https://cocodataset.org/#home), dataset of everyday images with 330k images and 80 catagories. The data is the segmantation of the objects.\n",
        "- [TACO](http://tacodataset.org/), data of trash in nature"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inq_9VTSrvb_",
        "outputId": "4539f3a4-1b5b-46c0-9d52-31a78644dd89"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/EEDL_046211/\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "!git pull\n",
        "%cd ./yolov5\n",
        "%pip install -qr requirements.txt\n",
        "%ls -l"
      ],
      "metadata": {
        "id": "pzY65phHw6-p",
        "outputId": "783e8a5b-57e8-41a7-9e4b-e411183c2ea7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/EEDL_046211\n",
            "fatal: destination path 'yolov5' already exists and is not an empty directory.\n",
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
            "/content/gdrive/MyDrive/EEDL_046211/yolov5\n",
            "total 14593\n",
            "-rw------- 1 root root     4991 Jan 19 10:01 CONTRIBUTING.md\n",
            "drwx------ 3 root root     4096 Jan 19 10:01 \u001b[0m\u001b[01;34mdata\u001b[0m/\n",
            "-rw------- 1 root root    13491 Jan 19 10:01 detect.py\n",
            "-rw------- 1 root root     2163 Jan 19 10:01 Dockerfile\n",
            "-rw------- 1 root root    25381 Jan 19 10:01 export.py\n",
            "-rw------- 1 root root     6386 Jan 19 10:01 hubconf.py\n",
            "-rw------- 1 root root    35127 Jan 19 10:01 LICENSE\n",
            "drwx------ 4 root root     4096 Jan 19 10:01 \u001b[01;34mmodels\u001b[0m/\n",
            "drwx------ 2 root root     4096 Jan 19 10:04 \u001b[01;34m__pycache__\u001b[0m/\n",
            "-rw------- 1 root root    15197 Jan 19 10:01 README.md\n",
            "-rw------- 1 root root      926 Jan 19 10:01 requirements.txt\n",
            "drwx------ 2 root root     4096 Jan 20 13:54 \u001b[01;34mruns\u001b[0m/\n",
            "-rw------- 1 root root      923 Jan 19 10:01 setup.cfg\n",
            "drwx------ 6 root root     4096 Jan 21 16:07 \u001b[01;34mSGD_fullcat_fz22\u001b[0m/\n",
            "drwx------ 2 root root     4096 Jan 20 21:34 \u001b[01;34mSGD_fullcat_fz24\u001b[0m/\n",
            "-rw------- 1 root root    33366 Jan 19 10:01 train.py\n",
            "-rw------- 1 root root    56455 Jan 19 10:01 tutorial.ipynb\n",
            "drwx------ 7 root root     4096 Jan 19 10:01 \u001b[01;34mutils\u001b[0m/\n",
            "-rw------- 1 root root    18890 Jan 19 10:01 val.py\n",
            "-rw------- 1 root root 14698491 Jan 19 10:05 yolov5s.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/EEDL_046211/\n",
        "!git clone https://github.com/pedropro/TACO\n",
        "!git pull\n",
        "%cd ./TACO\n",
        "%ls -l"
      ],
      "metadata": {
        "id": "cIuh9PUXvrwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/EEDL_046211/TACO/\n",
        "!python3 download.py"
      ],
      "metadata": {
        "id": "scgwwxlPw4UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/EEDL_046211/\n",
        "%pwd"
      ],
      "metadata": {
        "id": "8Mxtxh6CQWUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "scnIyi7W9u_r"
      },
      "outputs": [],
      "source": [
        "# imports for the tutorial\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%rm -fr ./datasets/taco\n",
        "#del dataset\n",
        "%cd /content/gdrive/MyDrive/EEDL_046211/TACO/detector/\n",
        "#from . import utils as utils_taco\n",
        "#from . import dataset as dataset_taco\n",
        "from utils import extract_bboxes\n",
        "from dataset import Taco\n",
        "\n",
        "def mkdir(dir_, dir):\n",
        "  d = os.path.join(dir_,dir)\n",
        "  if not os.path.isdir(d):\n",
        "    os.mkdir(d)\n",
        "  return d\n",
        "\n",
        "\n",
        "def get_class(i):\n",
        "  return int(i)\n",
        "\n",
        "dir = '../../datasets'\n",
        "dir_tc = mkdir(dir, 'taco')\n",
        "dir_im = mkdir(dir_tc, \"images\")\n",
        "dir_lb = mkdir(dir_tc, \"labels\")\n",
        "\n",
        "class_map = './taco_config/map_17.csv'\n",
        "with open(class_map) as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  class_map = {row[0]: row[1] for row in reader}\n",
        "\n",
        "print(class_map)\n",
        "subsets = [\"train\", \"val\"]\n",
        "\n",
        "for subset in subsets:\n",
        "  dir_im_ = mkdir(dir_im, subset)\n",
        "  dir_lb_ = mkdir(dir_lb, subset)\n",
        "  dataset = Taco()\n",
        "  dataset.load_taco('../data/', 0, subset, class_map=class_map)\n",
        "  dataset.prepare()\n",
        "  nrc = dataset.num_classes\n",
        "  faulty = 0\n",
        "\n",
        "  for idx in dataset._image_ids:\n",
        "    fn = dataset.image_info[idx]['path']\n",
        "    fn_im = os.path.dirname(fn).split('/')[-1] + \"_\" + os.path.basename(fn)\n",
        "    fn_lb = os.path.splitext(fn_im)[0] + \".txt\"\n",
        "\n",
        "    fn_im = os.path.join(dir_im_, fn_im)\n",
        "    fn_lb = os.path.join(dir_lb_, fn_lb)\n",
        "\n",
        "    if not os.path.isfile(fn_im):\n",
        "      shutil.copy(fn, fn_im)\n",
        "\n",
        "    ms , cl = dataset.load_mask(idx)\n",
        "    bbox = extract_bboxes(ms)\n",
        "    w_ , h_ = torch.tensor([dataset.image_info[idx]['width'] , dataset.image_info[idx]['height']])\n",
        "    f = open(fn_lb, 'w')\n",
        "    wr = csv.writer(f, delimiter= ' ')\n",
        "\n",
        "    for i in range(len(cl)):\n",
        "      c = get_class(cl[i])\n",
        "      y1, x1, y2, x2 = torch.Tensor(bbox[i])\n",
        "      #print(y1, x1, y2, x2)\n",
        "      x = float((x1 + x2) / (2 * w_))\n",
        "      y = float((y1 + y2) / (2 * h_))\n",
        "      w = float((x2 - x1) / w_)\n",
        "      h = float((y2 - y1) / h_)\n",
        "      if (x<=1 and x>=0) and(y<=1 and y>=0) and(w<=1 and w>=0) and(h<=1 and h>=0) and (c >= 0 and c < nrc):\n",
        "        wr.writerow([str(c)[:8], str(x)[:8], str(y)[:8], str(w)[:8], str(h)[:8]])\n",
        "      else:\n",
        "        faulty += 1\n",
        "    f.close()\n",
        "  pip uninstall utils\n",
        "\n",
        "#Yolo format : <class> <x-center> <y_center> <w> <h> normalized [0-1]\n",
        "#COCO format : <x-center> <y-center> <w> <h>"
      ],
      "metadata": {
        "id": "Sxw4aJq2k3WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/EEDL_046211/yolov5\n",
        "#%pip uninstall utils\n",
        "#del utils\n",
        "#from . import utils as utils_yolo\n",
        "from utils.datasets import create_dataloader\n",
        "from utils.plots import plot_images\n",
        "\n",
        "dataloader = create_dataloader('../datasets/taco/images/val', 900, 16, 1, False, pad=1, rect=True, workers=1, image_weights=True)\n",
        "\n",
        "for i, im in enumerate(dataloader[:3]):\n",
        "  print(len(im))\n",
        "  #plot_images(im, trg)\n"
      ],
      "metadata": {
        "id": "YfeM70La8XeT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce616add-55ea-4edd-af29-700e1723ce1e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/EEDL_046211/yolov5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning '../datasets/taco/labels/val.cache' images and labels... 150 found, 0 missing, 0 empty, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/EEDL_046211/yolov5/\n",
        "!python3 val.py --data ../taco.yaml --img 1216 --name check_bb_xyhw --weights ./SGD_fullcat_fz24/exp/weights/last.pt"
      ],
      "metadata": {
        "id": "c6o1JpWQ8XVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4838e781-f41b-4005-d153-39217c71cbab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/EEDL_046211/yolov5\n",
            "Traceback (most recent call last):\n",
            "  File \"val.py\", line 29, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/__init__.py\", line 197, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "RuntimeError: KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/EEDL_046211/yolov5/\n",
        "!python3 train.py --data ../taco.yaml --img 960 --batch 64 --epochs 32 --freeze 22  --project SGD_fullcat_fz22 --weights ./SGD_fullcat_fz22/exp6/weights/best.pt --cache"
      ],
      "metadata": {
        "id": "qYGNe08qCej7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cbd74db-0d97-4889-84c1-ec7a2d86d3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/EEDL_046211/yolov5\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=./SGD_fullcat_fz22/exp6/weights/best.pt, cfg=, data=../taco.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=32, batch_size=64, imgsz=960, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=SGD_fullcat_fz22, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=[22], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 8 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
            "YOLOv5 üöÄ v6.0-197-g0cf932b torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 üöÄ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir SGD_fullcat_fz22', view at http://localhost:6006/\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    186093  models.yolo.Detect                      [64, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 270 layers, 7192237 parameters, 7192237 gradients, 16.4 GFLOPs\n",
            "\n",
            "Transferred 349/349 items from SGD_fullcat_fz22/exp6/weights/best.pt\n",
            "freezing model.0.conv.weight\n",
            "freezing model.0.bn.weight\n",
            "freezing model.0.bn.bias\n",
            "freezing model.1.conv.weight\n",
            "freezing model.1.bn.weight\n",
            "freezing model.1.bn.bias\n",
            "freezing model.2.cv1.conv.weight\n",
            "freezing model.2.cv1.bn.weight\n",
            "freezing model.2.cv1.bn.bias\n",
            "freezing model.2.cv2.conv.weight\n",
            "freezing model.2.cv2.bn.weight\n",
            "freezing model.2.cv2.bn.bias\n",
            "freezing model.2.cv3.conv.weight\n",
            "freezing model.2.cv3.bn.weight\n",
            "freezing model.2.cv3.bn.bias\n",
            "freezing model.2.m.0.cv1.conv.weight\n",
            "freezing model.2.m.0.cv1.bn.weight\n",
            "freezing model.2.m.0.cv1.bn.bias\n",
            "freezing model.2.m.0.cv2.conv.weight\n",
            "freezing model.2.m.0.cv2.bn.weight\n",
            "freezing model.2.m.0.cv2.bn.bias\n",
            "freezing model.3.conv.weight\n",
            "freezing model.3.bn.weight\n",
            "freezing model.3.bn.bias\n",
            "freezing model.4.cv1.conv.weight\n",
            "freezing model.4.cv1.bn.weight\n",
            "freezing model.4.cv1.bn.bias\n",
            "freezing model.4.cv2.conv.weight\n",
            "freezing model.4.cv2.bn.weight\n",
            "freezing model.4.cv2.bn.bias\n",
            "freezing model.4.cv3.conv.weight\n",
            "freezing model.4.cv3.bn.weight\n",
            "freezing model.4.cv3.bn.bias\n",
            "freezing model.4.m.0.cv1.conv.weight\n",
            "freezing model.4.m.0.cv1.bn.weight\n",
            "freezing model.4.m.0.cv1.bn.bias\n",
            "freezing model.4.m.0.cv2.conv.weight\n",
            "freezing model.4.m.0.cv2.bn.weight\n",
            "freezing model.4.m.0.cv2.bn.bias\n",
            "freezing model.4.m.1.cv1.conv.weight\n",
            "freezing model.4.m.1.cv1.bn.weight\n",
            "freezing model.4.m.1.cv1.bn.bias\n",
            "freezing model.4.m.1.cv2.conv.weight\n",
            "freezing model.4.m.1.cv2.bn.weight\n",
            "freezing model.4.m.1.cv2.bn.bias\n",
            "freezing model.5.conv.weight\n",
            "freezing model.5.bn.weight\n",
            "freezing model.5.bn.bias\n",
            "freezing model.6.cv1.conv.weight\n",
            "freezing model.6.cv1.bn.weight\n",
            "freezing model.6.cv1.bn.bias\n",
            "freezing model.6.cv2.conv.weight\n",
            "freezing model.6.cv2.bn.weight\n",
            "freezing model.6.cv2.bn.bias\n",
            "freezing model.6.cv3.conv.weight\n",
            "freezing model.6.cv3.bn.weight\n",
            "freezing model.6.cv3.bn.bias\n",
            "freezing model.6.m.0.cv1.conv.weight\n",
            "freezing model.6.m.0.cv1.bn.weight\n",
            "freezing model.6.m.0.cv1.bn.bias\n",
            "freezing model.6.m.0.cv2.conv.weight\n",
            "freezing model.6.m.0.cv2.bn.weight\n",
            "freezing model.6.m.0.cv2.bn.bias\n",
            "freezing model.6.m.1.cv1.conv.weight\n",
            "freezing model.6.m.1.cv1.bn.weight\n",
            "freezing model.6.m.1.cv1.bn.bias\n",
            "freezing model.6.m.1.cv2.conv.weight\n",
            "freezing model.6.m.1.cv2.bn.weight\n",
            "freezing model.6.m.1.cv2.bn.bias\n",
            "freezing model.6.m.2.cv1.conv.weight\n",
            "freezing model.6.m.2.cv1.bn.weight\n",
            "freezing model.6.m.2.cv1.bn.bias\n",
            "freezing model.6.m.2.cv2.conv.weight\n",
            "freezing model.6.m.2.cv2.bn.weight\n",
            "freezing model.6.m.2.cv2.bn.bias\n",
            "freezing model.7.conv.weight\n",
            "freezing model.7.bn.weight\n",
            "freezing model.7.bn.bias\n",
            "freezing model.8.cv1.conv.weight\n",
            "freezing model.8.cv1.bn.weight\n",
            "freezing model.8.cv1.bn.bias\n",
            "freezing model.8.cv2.conv.weight\n",
            "freezing model.8.cv2.bn.weight\n",
            "freezing model.8.cv2.bn.bias\n",
            "freezing model.8.cv3.conv.weight\n",
            "freezing model.8.cv3.bn.weight\n",
            "freezing model.8.cv3.bn.bias\n",
            "freezing model.8.m.0.cv1.conv.weight\n",
            "freezing model.8.m.0.cv1.bn.weight\n",
            "freezing model.8.m.0.cv1.bn.bias\n",
            "freezing model.8.m.0.cv2.conv.weight\n",
            "freezing model.8.m.0.cv2.bn.weight\n",
            "freezing model.8.m.0.cv2.bn.bias\n",
            "freezing model.9.cv1.conv.weight\n",
            "freezing model.9.cv1.bn.weight\n",
            "freezing model.9.cv1.bn.bias\n",
            "freezing model.9.cv2.conv.weight\n",
            "freezing model.9.cv2.bn.weight\n",
            "freezing model.9.cv2.bn.bias\n",
            "freezing model.10.conv.weight\n",
            "freezing model.10.bn.weight\n",
            "freezing model.10.bn.bias\n",
            "freezing model.13.cv1.conv.weight\n",
            "freezing model.13.cv1.bn.weight\n",
            "freezing model.13.cv1.bn.bias\n",
            "freezing model.13.cv2.conv.weight\n",
            "freezing model.13.cv2.bn.weight\n",
            "freezing model.13.cv2.bn.bias\n",
            "freezing model.13.cv3.conv.weight\n",
            "freezing model.13.cv3.bn.weight\n",
            "freezing model.13.cv3.bn.bias\n",
            "freezing model.13.m.0.cv1.conv.weight\n",
            "freezing model.13.m.0.cv1.bn.weight\n",
            "freezing model.13.m.0.cv1.bn.bias\n",
            "freezing model.13.m.0.cv2.conv.weight\n",
            "freezing model.13.m.0.cv2.bn.weight\n",
            "freezing model.13.m.0.cv2.bn.bias\n",
            "freezing model.14.conv.weight\n",
            "freezing model.14.bn.weight\n",
            "freezing model.14.bn.bias\n",
            "freezing model.17.cv1.conv.weight\n",
            "freezing model.17.cv1.bn.weight\n",
            "freezing model.17.cv1.bn.bias\n",
            "freezing model.17.cv2.conv.weight\n",
            "freezing model.17.cv2.bn.weight\n",
            "freezing model.17.cv2.bn.bias\n",
            "freezing model.17.cv3.conv.weight\n",
            "freezing model.17.cv3.bn.weight\n",
            "freezing model.17.cv3.bn.bias\n",
            "freezing model.17.m.0.cv1.conv.weight\n",
            "freezing model.17.m.0.cv1.bn.weight\n",
            "freezing model.17.m.0.cv1.bn.bias\n",
            "freezing model.17.m.0.cv2.conv.weight\n",
            "freezing model.17.m.0.cv2.bn.weight\n",
            "freezing model.17.m.0.cv2.bn.bias\n",
            "freezing model.18.conv.weight\n",
            "freezing model.18.bn.weight\n",
            "freezing model.18.bn.bias\n",
            "freezing model.20.cv1.conv.weight\n",
            "freezing model.20.cv1.bn.weight\n",
            "freezing model.20.cv1.bn.bias\n",
            "freezing model.20.cv2.conv.weight\n",
            "freezing model.20.cv2.bn.weight\n",
            "freezing model.20.cv2.bn.bias\n",
            "freezing model.20.cv3.conv.weight\n",
            "freezing model.20.cv3.bn.weight\n",
            "freezing model.20.cv3.bn.bias\n",
            "freezing model.20.m.0.cv1.conv.weight\n",
            "freezing model.20.m.0.cv1.bn.weight\n",
            "freezing model.20.m.0.cv1.bn.bias\n",
            "freezing model.20.m.0.cv2.conv.weight\n",
            "freezing model.20.m.0.cv2.bn.weight\n",
            "freezing model.20.m.0.cv2.bn.bias\n",
            "freezing model.21.conv.weight\n",
            "freezing model.21.bn.weight\n",
            "freezing model.21.bn.bias\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/taco/labels/train.cache' images and labels... 1350 found, 0 missing, 0 empty, 0 corrupt: 100% 1350/1350 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB ram):  15% 208/1350 [00:48<08:31,  2.23it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-7rWvNSc8XJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grave yard"
      ],
      "metadata": {
        "id": "rccL18m78Zp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "# optuna\n",
        "import optuna\n"
      ],
      "metadata": {
        "id": "YUn2OHftHly9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -fr ./datasets/taco\n",
        "ann_j = \"./TACO/data/annotations.json\"\n",
        "im_dir = \"./TACO/data/\"\n",
        "new_dir = \"./datasets/taco\"\n",
        "ds = json.load(open(ann_j, 'r'))\n",
        "print(ds['images'][0].keys())\n",
        "images = {x['id']: {'fn': x['file_name'], 'size': (x['width'], x['height'])} for x in ds['images']}\n",
        "annots = [{'class' : x['category_id'], 'im_id': x['image_id'], 'bbox': x['bbox']} for x in ds['annotations']]\n",
        "\n",
        "print(\"There are {} images and {} annotations\".format(len(images), len(annots)))\n",
        "print(annots[:3])\n",
        "idx_train = sorted(np.random.choice(len(images), size=int(0.9*len(images)), replace=False))\n",
        "idx_val = [x for x in range(len(images)) if x not in idx_train]\n",
        "\n",
        "idx_type = [[\"train\", idx_train], ['val', idx_val]]\n",
        "os.mkdir(new_dir)\n",
        "dir_im_ = os.path.join(new_dir, \"images\")\n",
        "dir_lb_ = os.path.join(new_dir, \"labels\")\n",
        "os.mkdir(dir_im_)\n",
        "os.mkdir(dir_lb_)\n",
        "for ds_type, idxs in idx_type:\n",
        "  dir_im = os.path.join(dir_im_, ds_type)\n",
        "  dir_lb = os.path.join(dir_lb_, ds_type)\n",
        "  os.mkdir(dir_im)\n",
        "  os.mkdir(dir_lb)  \n",
        "  for idx in idx_train:\n",
        "    fn_im = images[idx]['fn']\n",
        "    fn_n_ = '_'.join(fn_im.split('/'))\n",
        "    fn_n = os.path.join(dir_im, fn_n_)\n",
        "    fn_o = os.path.join(im_dir, fn_im)\n",
        "\n",
        "    shutil.copy(fn_o, fn_n)\n",
        "    \n",
        "    wd, hg = images[idx]['size']\n",
        "    \n",
        "    lbls = [x for x in annots if x['im_id'] == idx]\n",
        "    fn_l = os.path.splitext(fn_n_)[0] + \".txt\"\n",
        "    fn_l = os.path.join(dir_lb, fn_l)\n",
        "    f = open(fn_l, 'w')\n",
        "    wr = csv.writer(f, delimiter= ' ')\n",
        "\n",
        "    for l in lbls:\n",
        "      r = [str(l['class']), \n",
        "           str(l['bbox'][0] / wd)[:8], \n",
        "           str(l['bbox'][1] / hg)[:8], \n",
        "           str(l['bbox'][2] / wd)[:8], \n",
        "           str(l['bbox'][3] / hg)[:8]]\n",
        "      wr.writerow(r)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "#Yolo format : <class> <x-center> <y_center> <w> <h> normalized [0-1]\n",
        "#COCO format : <x-center> <y-center> <w> <h>"
      ],
      "metadata": {
        "id": "8-jVqTN8_6Gl",
        "outputId": "fcb28853-c65e-4f05-fcea-4a7efdf5d52c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'width', 'height', 'file_name', 'license', 'flickr_url', 'coco_url', 'date_captured', 'flickr_640_url'])\n",
            "There are 1500 images and 4784 annotations\n",
            "[{'class': 6, 'im_id': 0, 'bbox': [517.0, 127.0, 447.0, 1322.0]}, {'class': 18, 'im_id': 1, 'bbox': [1.0, 457.0, 1429.0, 1519.0]}, {'class': 14, 'im_id': 1, 'bbox': [531.0, 292.0, 1006.0, 672.0]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Taco_ds():\n",
        "  reader = csv.reader(csv_file)\n",
        "  class_map = {row[0]: row[1] for row in reader}\n",
        "  map_to_one_class = {c: 'Litter' for c in class_map}\n",
        "  dataset = \"./TACO/data/\"\n",
        "\n",
        "  # Training dataset.\n",
        "  dataset_train = Taco()\n",
        "  dataset_train.load_taco(dataset, 1, \"train\", class_map=class_map, auto_download=None)\n",
        "  dataset_train.prepare()\n",
        "  nr_classes = dataset_train.num_classes\n",
        "  # Validation dataset\n",
        "  dataset_val = Taco()\n",
        "  dataset_val.load_taco(dataset, 1, \"val\", class_map=class_map, auto_download=None)\n",
        "  dataset_val.prepare()\n",
        "\n",
        "  return dataset_train, dataset_val\n",
        "\n",
        "\n",
        "tr, vl = Taco_ds()\n",
        "\n",
        "class TACO_DataSet(Dataset):\n",
        "  def __init__(self, img_dir, ann_j, class_map, transform=None, target_transform=None):\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    ds = json.load(open(ann_j, 'r'))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return 0\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = []\n",
        "    bbox = []\n",
        "\n",
        "    return image, bbox"
      ],
      "metadata": {
        "id": "349RG6yg2mVR",
        "outputId": "a013e923-8389-4ba0-e22c-7e1073052084",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating index...\n",
            "index created!\n",
            "Number of images used: 1200\n",
            "creating index...\n",
            "index created!\n",
            "Number of images used: 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GIoU(nn.Module):\n",
        "  def __init__(self):\n",
        "    self.eq = 0\n",
        "\n",
        "  def forward(pred, gt):\n",
        "    return 0\n",
        "\n",
        "\n",
        "def stepLrSch(opt):\n",
        "  return torch.optim.lr_scheduler.StepLR(opt, 2, gamma=0.9)\n",
        "\n",
        "\n",
        "def default_transform():\n",
        "  return transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])"
      ],
      "metadata": {
        "id": "EzZgKJxf4WO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HP():\n",
        "  def __init__(self, name=None, epochs=16, btz=128, lr=1e-3, opt=None, crit=None, sch=None, bt_tr=None):\n",
        "    self.name = datetime.datetime.now().strftime(\"%Y-%m-%d--%H,%M\")\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = btz\n",
        "    self.lr = lr\n",
        "    self.optimizer = opt if opt is not None else torch.optim.Adam\n",
        "    self.criterion = crit if crit is not None else GIou()\n",
        "    self.scheduler = sch if sch is not None else stepLrSch\n",
        "    self.bt_transform = bt_tr if bt_tr is not None else default_transform()\n",
        "  \n",
        "  def get_optimizer(model):\n",
        "    self.optimizer = self.optimizer(model.parameters(), lr=self.lr)\n",
        "    return self.optimizer\n",
        "\n",
        "  def get_criterion():\n",
        "    return self.criterion\n",
        "\n",
        "  def get_scheduler():\n",
        "    self.scheduler = self.scheduler(self.optimizer)\n",
        "    return self.scheduler\n",
        "\n",
        "  def export():\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "ZdScZcNv-rKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TACO_model(classes=25, freeze=1):  \n",
        "  yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, classes=classes)\n",
        "  for m in [m_ for m_ in yolo.modules()][:-freeze]:\n",
        "    m.requires_grad_(False)\n",
        "  return yolo\n"
      ],
      "metadata": {
        "id": "XlVyLpvF519E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DL_model():\n",
        "  def __init__(self, hp=None):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.model = TACO_model().to(self.device)\n",
        "    self.hp = hp if hp is not None else HP()\n",
        "    self.optimizer = hp.get_optimizer(self.model)\n",
        "    self.criterion = hp.get_criterion()\n",
        "    self.scheduler = hp.get_scheduler()\n",
        "\n",
        "  def evaluate():\n",
        "    return 0.0\n",
        "\n",
        "  def train():\n",
        "    return None"
      ],
      "metadata": {
        "id": "uaHc3i8s0rgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ElytPmoTrOS",
        "outputId": "26a472f0-37a4-4fa3-9565-d0e08575baa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-422d6acb381d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ultralytics/yolov5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yolov5s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mrepo_or_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_cache_or_reload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\u001b[0m in \u001b[0;36myolov5s\u001b[0;34m(pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0myolov5s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# YOLOv5-small model https://github.com/ultralytics/yolov5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolov5s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoShape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDetectMultiBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloads\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattempt_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexif_transpose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletterbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m from utils.general import (LOGGER, check_requirements, check_suffix, check_version, colorstr, increment_path,\n\u001b[1;32m     26\u001b[0m                            make_divisible, non_max_suppression, scale_coords, xywh2xyxy, xyxy2xywh)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.datasets'; 'utils' is not a package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = 'https://ultralytics.com/images/'\n",
        "imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n",
        "\n",
        "# Inference\n",
        "results = model(imgs)\n",
        "results.print()  # or .show(), .save()\n",
        "results.pandas().xyxy[1]"
      ],
      "metadata": {
        "id": "ok_POtMRJGZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QSIZJaRQKpPb",
        "outputId": "288de7a9-04c5-434a-c302-c8c711311058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-596b1a45-c938-4aec-8049-dc2836945238\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>confidence</th>\n",
              "      <th>class</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>223.462814</td>\n",
              "      <td>404.009460</td>\n",
              "      <td>345.418243</td>\n",
              "      <td>867.322144</td>\n",
              "      <td>0.860068</td>\n",
              "      <td>0</td>\n",
              "      <td>person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>668.152039</td>\n",
              "      <td>401.405701</td>\n",
              "      <td>808.515930</td>\n",
              "      <td>882.466370</td>\n",
              "      <td>0.826754</td>\n",
              "      <td>0</td>\n",
              "      <td>person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50.385479</td>\n",
              "      <td>398.525116</td>\n",
              "      <td>242.990524</td>\n",
              "      <td>905.033630</td>\n",
              "      <td>0.808898</td>\n",
              "      <td>0</td>\n",
              "      <td>person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23.899151</td>\n",
              "      <td>215.100525</td>\n",
              "      <td>801.807922</td>\n",
              "      <td>779.779175</td>\n",
              "      <td>0.757313</td>\n",
              "      <td>5</td>\n",
              "      <td>bus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.742684</td>\n",
              "      <td>551.577087</td>\n",
              "      <td>78.647270</td>\n",
              "      <td>891.026184</td>\n",
              "      <td>0.355743</td>\n",
              "      <td>0</td>\n",
              "      <td>person</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-596b1a45-c938-4aec-8049-dc2836945238')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-596b1a45-c938-4aec-8049-dc2836945238 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-596b1a45-c938-4aec-8049-dc2836945238');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         xmin        ymin        xmax        ymax  confidence  class    name\n",
              "0  223.462814  404.009460  345.418243  867.322144    0.860068      0  person\n",
              "1  668.152039  401.405701  808.515930  882.466370    0.826754      0  person\n",
              "2   50.385479  398.525116  242.990524  905.033630    0.808898      0  person\n",
              "3   23.899151  215.100525  801.807922  779.779175    0.757313      5     bus\n",
              "4    0.742684  551.577087   78.647270  891.026184    0.355743      0  person"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjAiMX1l9u_s"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
        "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
        "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "ee046211_project.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}