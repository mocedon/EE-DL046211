{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzV9wsJ5pGhf"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> EE 046211 - Technion - Deep Learning\n",
        "---\n",
        "\n",
        "## HW2 - Multilayer NNs and Convolutional NNs\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq2c8X93pGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
        "---\n",
        "* Run current cell: **Ctrl + Enter**\n",
        "* Run current cell and move to the next: **Shift + Enter**\n",
        "* Show lines in a code cell: **Esc + L**\n",
        "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
        "* New cell below: **Esc + B**\n",
        "* Delete cell: **Esc + D, D** (two D's)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZZybn3NpGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
        "---\n",
        "* Fill in\n",
        "\n",
        "|Name     |Campus Email| ID  |\n",
        "|---------|--------------------------------|----------|\n",
        "|Alexander Balabanov| alexander.b@campus.technion.ac.il| 312775364|\n",
        "|Student 2| student_2@campus.technion.ac.il| 987654321|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDK5zqhdpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
        "---\n",
        "* Maximal garde: 100.\n",
        "* Submission only in **pairs**. \n",
        "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
        "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
        "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
        "* What you have to submit:\n",
        "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046211_hw2_id1_id2.ipynb`.\n",
        "    * If you answered the questions in a different file you should submit a `.zip` file with the name `ee046211_hw2_id1_id2.zip` with content:\n",
        "        * `ee046211_hw2_id1_id2.ipynb` - the code tasks\n",
        "        * `ee046211_hw2_id1_id2.pdf` - answers to questions.\n",
        "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
        "* Submission on the course website (Moodle).\n",
        "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmSj_UufpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
        "---\n",
        "* You can choose your working environment:\n",
        "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
        "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
        "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
        "        * Both allow editing and running Jupyter Notebooks.\n",
        "\n",
        "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.\n",
        "* If you need any technical assistance, please go to our Piazza forum (`hw2` folder) and describe your problem (preferably with images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlp1Fp4ppGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
        "---\n",
        "\n",
        "* [Part 1 - Theory](#-Part-1---Theory)\n",
        "    * [Q1 - Generalization in A Teacher-Student Setup](#-Question-1--Generalization-in-A-Teacher-Student-Setup)\n",
        "    * [Q2 - Backpropagation By Hand](#-Question-2---Backpropagation-By-Hand)\n",
        "    * [Q3 - Deep Double Descent](#-Question-3---Deep-Double-Descent)\n",
        "    * [Q4 - Initialization](#-Question-4---Initialization)\n",
        "    * [Q5 - MLP and Invaraince](#-Question-5---MLP-and-Invaraince)\n",
        "    * [Q6 - VGG Architecture](#-Question-6--VGG-Architecture)\n",
        "* [Part 2 - Code Assignments](#-Part-2---Code-Assignments)\n",
        "    * [Task 1 - The Importance of Activation and Initialization](#-Task-1---The-Importance-of-Activation-and-Initialization)\n",
        "    * [Task 2 - FashionMNIST Deep Classifer](#-Task-2---FashionMNIST-Deep-Classifer)\n",
        "    * [Task 3 - Design a CNN](#-Task-3---Design-a-CNN)\n",
        "* [Credits](#-Credits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKtSiQX_pGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png\" style=\"height:50px;display:inline\"> Part 1 - Theory\n",
        "---\n",
        "* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.\n",
        "* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.\n",
        "\n",
        "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
        "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsqSFZG1pGhj"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 1 -Generalization in A Teacher-Student Setup\n",
        "---\n",
        "\n",
        "Recall from lecture 4 the Risk $\\mathcal{R}(w)$: $$ \\mathcal{R}(w) \\triangleq \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||w^Tx^{(0)} - w_t^Tx^{(0)}||^2 \\right] $$\n",
        "\n",
        "Prove:\n",
        "\n",
        "$$ \\mathcal{R}(w) = ||w-w_t||^2 $$\n",
        "\n",
        "\n",
        "# Answer\n",
        "Expanding the right term\n",
        "$$ \\mathcal{R}(w) \\triangleq \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||w^Tx^{(0)} - w_t^Tx^{(0)}||^2 \\right] =\n",
        "\\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) }\\left[(w^Tx^{(0)}-w_t^Tx^{(0)})(w^Tx^{(0)}-w_t^Tx^{(0)})^T\n",
        "  \\right] =$$\n",
        "  Using the expansion and determinate nature of $w, w_t$:\n",
        "  $$= w^T\\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) }\\left[\n",
        "  x^{(0)}x^{(0)T}\\right]w-$$\n",
        "  $$- w^T\\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) }\\left[\n",
        "  x^{(0)}x^{(0)T}\\right]w_t-$$\n",
        "  $$- w^T_t\\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) }\\left[\n",
        "  x^{(0)}x^{(0)T}\\right]w+$$\n",
        "  $$+ w^T_t\\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) }\\left[\n",
        "  x^{(0)}x^{(0)T}\\right]w^T_t=$$\n",
        "This term is the MSE that is also the variance for $\\mu=0$:\n",
        "  $$ \\begin{Bmatrix}\\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) }\\left[x^{(0)}x^{(0)T}\n",
        "  \\right] = \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) }\\left[||x^{(0)}||^2\n",
        "  \\right]=I \\end{Bmatrix}$$\n",
        "Resulting in:\n",
        "  $$ =w^Tw -w^Tw_t - w^Tw + w^Tw_t =||w-w_t||^2 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7-nAbsT4D2K"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 2 - Backpropagation By Hand\n",
        "---\n",
        "Consider the following network:\n",
        "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/backprop_by_hand_ex1.png\" style=\"height:300px\">\n",
        "\n",
        "We will work with one sample for this example, but it can be extended to mini-batches.\n",
        "\n",
        "* Input: $x = \\begin{bmatrix} 1 \\\\ 4 \\\\ 5 \\end{bmatrix} \\in \\mathbb{R}^3$\n",
        "* Output (target): $ t = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix} \\in \\mathbb{R}^2 $\n",
        "* Number of Hidden Layers: 1\n",
        "* Activation: Sigmoid for both hidden and output layers\n",
        "* Loss Functions: MSE\n",
        "\n",
        "We initialize the weights and biases to random values as follows:\n",
        "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/backprop_by_hand_ex2.png\" style=\"height:300px\">\n",
        "\n",
        "1. Perform one forward pass and calculate the MSE.\n",
        "2. Perform backpropagation (one backward pass, i.e., calculate the gradients).\n",
        "3. With a learning rate of $\\alpha = 0.01$, what are the new values of the weights after performing the forward pass and backward pass (assume we use SGD)?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Answer\n",
        "  * Part 1.\n",
        "\n",
        "  Calculating forward pass\n",
        "$$h_1 = σ(x_1w_1+x_2w_3+x_3w_5+b_1) = σ(4.3)=0.986$$\n",
        "$$h_2 = σ(x_1w_2+x_2w_4+x_3w_6+b_1) = σ(5.3)=0.995$$\n",
        "$$o_1 = σ(h_1w_7+h_2w_{9~}+b_2) = σ(2.088)=0.890$$\n",
        "$$o_2 = σ(h_1w_8+h_2w_{10}+b_1) = σ(1.389)=0.800$$\n",
        "$$l = \\frac{(0.1-o_1)^2+(0.05-o_2)^2}{2}=0.593$$\n",
        "\n",
        "  * Part 2.\n",
        "\n",
        "  First let us find the known derivatives:\n",
        "  $$\\frac{∂}{∂o_i}l= (o_i-t_i) $$\n",
        "  $$\\frac{∂}{∂w_i}o_j=\\frac{∂}{∂w_i}σ(o_i')=\\frac{∂}{∂o'_i}\\sigma(o'_i)⋅\\frac{∂}{∂w_i}o'_i=σ(o'_i)(1-σ(o'_i))h_k = σ'(o'_i)h_k $$\n",
        "  $$\\frac{∂}{∂h_i}o_j=\\frac{∂}{∂h_i}σ(o_i')=\\frac{∂}{∂o'_i}\\sigma(o'_i)⋅\\frac{∂}{∂h_i}o'_i=σ(o'_i)(1-σ(o'_i))w_k = σ'(o'_i)w_k $$\n",
        "  $$\\frac{∂}{∂b_i}o_j=\\frac{∂}{∂b_i}σ(o_i')=\\frac{∂}{∂o'_i}\\sigma(o'_i)⋅\\frac{∂}{∂b_i}o'_i=σ(o'_i)(1-σ(o'_i))\\cdot 1 = σ'(o'_i) $$\n",
        "  These derivatives covers the whole tree,\n",
        "  \n",
        "  Moving back on the tree:\n",
        "  $$ \\frac{∂}{∂w_7}l=\\frac{∂}{∂o_1}l\\cdot \\frac{∂}{∂w_7}o_1 =(0.79)σ'(o'_1)h_1=0.076 $$\n",
        "  $$ \\frac{∂}{∂w_8}l=\\frac{∂}{∂o_2}l\\cdot \\frac{∂}{∂w_8}o_2 =(0.75)σ'(o'_2)h_1=0.118$$\n",
        "  $$ \\frac{∂}{∂w_9}l=\\frac{∂}{∂o_1}l\\cdot \\frac{∂}{∂w_7}o_1=(0.79)σ'(o'_1)h_2=0.077$$\n",
        "  $$ \\frac{∂}{∂w_{10}}l=\\frac{∂}{∂o_2}l\\cdot \\frac{∂}{∂w_{10}}o_2=(0.75)σ'(o'_2)h_2=0.119$$\n",
        "  \n",
        "  $$ \\frac{∂}{∂b_2}l=\\frac{∂}{∂o_1}l\\cdot \\frac{∂}{∂b_2}o_1 + \n",
        "  \\frac{∂}{∂o_2}l\\cdot \\frac{∂}{∂b_2}o_2 = (0.79)σ'(o'_1)+(0.75)σ'(o'_2)=0.197$$\n",
        "\n",
        "  $$ \\frac{∂}{∂h_1}l=\\frac{∂}{∂o_1}l\\cdot \\frac{∂}{∂h_1}o_1 + \n",
        "  \\frac{∂}{∂o_2}l\\cdot \\frac{∂}{∂h_1}o_2 = (0.79)σ'(o'_1)w_7+(0.75)σ'(o'_2)w_8=0.150$$\n",
        "  $$ \\frac{∂}{∂h_2}l=\\frac{∂}{∂o_1}l\\cdot \\frac{∂}{∂h_2}o_1 + \n",
        "  \\frac{∂}{∂o_2}l\\cdot \\frac{∂}{∂h_2}o_2=(0.79)σ'(o'_1)w_9 +(0.75)σ'(o'_2)w_{10}=0.082$$\n",
        "  \n",
        "  $$ \\frac{∂l}{∂w_1}=\\frac{∂l}{∂h_1}\\cdot \\frac{∂h_1}{∂w_1} =(0.15)σ'(h'_1)x_1=0.002$$\n",
        "  $$ \\frac{∂l}{∂w_2}=\\frac{∂l}{∂h_2}\\cdot \\frac{∂h_2}{∂w_2}=(0.08)σ'(h'_2)x_2=0.0004$$\n",
        "  $$ \\frac{∂l}{∂w_3}=\\frac{∂l}{∂h_1}\\cdot \\frac{∂h_1}{∂w_3}=(0.15)σ'(h'_1)x_3=0.008$$\n",
        "  $$ \\frac{∂l}{∂w_4}=\\frac{∂l}{∂h_2}\\cdot \\frac{∂h_2}{∂w_4}=(0.08)σ'(h'_2)x_4=0.0016$$\n",
        "  $$ \\frac{∂l}{∂w_5}=\\frac{∂l}{∂h_1}\\cdot \\frac{∂h_1}{∂w_5}=(0.15)σ'(h'_1)x_5=0.001$$\n",
        "  $$ \\frac{∂l}{∂w_6}=\\frac{∂l}{∂h_2}\\cdot \\frac{∂h_2}{∂w_6}=(0.08)σ'(h'_2)x_6=0.002$$\n",
        "  $$ \\frac{∂l}{∂b_1}=\\frac{∂l}{∂h_1}\\cdot \\frac{∂h_1}{∂b_1} + \\frac{∂l}{∂h_2}\\cdot \\frac{∂h_2}{∂b_1}=(0.15)σ'(h'_1)+(0.08)σ'(h'_2)=0.002$$\n",
        "\n",
        "  * Part 3.\n",
        "\n",
        "  Using SGD for batch size of 1, the update policy is:\n",
        "  $$ w(t+1) = w(t) -α\\cdot ∇l(f(x,w)) $$\n",
        "  Therefore the new weights are:\n",
        "  $$ w_1(1) = 0.0998 $$\n",
        "  $$ w_2(1) = 0.199996 $$\n",
        "  $$ w_3(1) = 0.299 $$\n",
        "  $$ w_4(1) = 0.399 $$\n",
        "  $$ w_5(1) = 0.499 $$\n",
        "  $$ w_6(1) = 0.599 $$\n",
        "  $$ b_1(1) = 0.499 $$\n",
        "  $$ w_7(1) = 0.699 $$\n",
        "  $$ w_8(1) = 0.798 $$\n",
        "  $$ w_9(1) = 0.899 $$\n",
        "  $$ w_{10}(1) = 0.0988 $$\n",
        "  $$ b_2(1) = 0.498 $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i33SnLB84D2M"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 3 - Deep Double Descent\n",
        "---\n",
        "\n",
        "For the following plots:\n",
        "1. Where is the critical point (the point of transition between the \"Classical Regime\" and \"Modern Regime\") of the deep double descent?\n",
        "2. What type of double descent is shown? Explain.\n",
        "    \n",
        "\n",
        "a. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_transformer.PNG' style=\"height:300px\">\n",
        "\n",
        "Figure 1:\n",
        "\n",
        "The critical point is different for both models,\n",
        "\n",
        "De-En - critical point is around 200\n",
        "En-Fr - critical point is around 280\n",
        "\n",
        "Since we have critical point that moves for En-Fr we would deduce there are more samples and it causes the move, is it Sample-wise non-monotonicity\n",
        "\n",
        "\n",
        "b. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_resnet.PNG' style=\"height:300px\">\n",
        "\n",
        "All 3 noise model have the same critical point around 12 width parameter, which makes sense since noise doesn't affect the point of switch between model.\n",
        "\n",
        "Again it is model-wise double descent since the width parameter changes how big the network is and how many parameters are there.\n",
        "\n",
        "\n",
        "c. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_intermediate.PNG' style=\"height:300px\">\n",
        "\n",
        "The critical point as on the large model, around 100 epochs.\n",
        "The small model can't fit all data so it doesn't experience the bias-variance treadeoff.\n",
        "Medium model can't fir the entire model and therefore stuck in the bias-variance regime.\n",
        "\n",
        "As suggested by the X axis, is in epoch-wise double descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BLFOJWN4D2N"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 4 - Initialization\n",
        "---\n",
        "\n",
        "Recall that in lecture 5 we were discussing how to calculate the initialization variance, and reached the conclusion that $$ \\sigma_l =\\frac{1}{\\sqrt{d_{l-1}\\mathbb{E}_{z\\sim \\mathcal{N}(0, 1)} \\left[\\varphi^2(z)\\right]}} $$\n",
        "Show that for ReLU activation ($\\varphi(z) = max(0,z)$), the optimal variance satisfies: $$ \\sigma_l = \\sqrt{\\frac{2}{d_{l-1}}}$$\n",
        "\n",
        "All the notations are the same as in the lecture slides.\n",
        "\n",
        "# Answer\n",
        "Let us find the mean of the square of the activation function\n",
        "$$\\mathbb{E}_{z\\sim \\mathcal{N}(0, 1)} \\left[\\varphi^2(z)\\right]=\\int_{-\\infty}^{\\infty}f(z)ϕ^2(z)dz=\\int_{-\\infty}^{0}f(z)ϕ^2(z)dz+\\int_{0}^{\\infty}f(z)ϕ^2(z)dz\\underset{(1)}{=}\\int_{-\\infty}^{0}f(z)\\cdot 0dz+\\int_{0}^{\\infty}f(z)z^2dz\\underset{(2)}{=}\\frac{1}{2}\\int_{-\\infty}^{\\infty}f(z)z^2dz=\\frac{1}{2}var(z)=\\frac{1}{2}$$\n",
        "\n",
        "Where:\n",
        "$$(1) ~~~ϕ(z)=\\cases{z & z≥ 0 \\\\ 0 & z<0}$$\n",
        "$$(2) ~~~g(z)=f(z)z^2=g(-z)$$\n",
        "\n",
        "$$σ_l=\\frac{1}{\\sqrt{d_{l-1}\\mathbb{E}_{z\\sim \\mathcal{N}(0, 1)} \\left[\\varphi^2(z)\\right]}}=\\frac{1}{\\sqrt{\\frac{d_{l-1}}{2}}}=\\sqrt{\\frac{2}{d_{l-1}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cmyjbx34D2N"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 5 - MLP and Invaraince\n",
        "---\n",
        "\n",
        "You have to design an MLP with the following input: DNA sequences of length $d$. The DNA is a sequence of bases, where each base can be one of 4 options: $(C, T, G, A)$. Thus, the input can be described as the following matrix: $$ X \\in \\mathcal{R}^{4 \\times d}, $$ where $X[j,i]$ denotes the measured value of base concentration of the $j^{th}$ base at location $i$. \n",
        "\n",
        "The network should output a **binary** classification $y \\in \\{-1, 1\\}$ for a specific property we wish to find. The network will be trained on samples $\\{X^{(n)}, y^{(n)} \\}_{n=1}^{N}$, with a **logistic loss function**.\n",
        "\n",
        "First, we will examine a network with 1 hidden layerof size $4 \\times d$ and a **LeakyReLU** activation $\\phi$: $$ f_w(X) = \\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]X[j, i] \\right),$$ where $w=\\{W_1, W_2\\}$ are the layers of the weight **tensors**. After training is done, the classification will be done with $\\text{sign}(f(X))$.\n",
        "\n",
        "1. Which invariances exist in the network's parameters?\n",
        "2. Now, we notice the fact that: the *direction* in which the DNA is scanned is arbitrary. Thus, if for two inputs $X, \\tilde{X}$: $$ \\forall i,j: \\: X[j,i] = \\tilde{X}[j, d-i+1], $$ then the two inputs are **equivalent** in their meaning. What constraints should we put on the network's parameters to improve the network's classification performance?\n",
        "3. After that, we now recall that the DNA bases come in pairs, and thus if for two inputs $X, \\tilde{X}$: $$ \\forall i,j : \\: X[j,i] = \\tilde{X}[(j+2)\\text{mod}4 + 1,i], $$ then the two inputs are **equivalent** in their meaning. What constraints should we put on the network's parameters to improve the network's classification performance?\n",
        "4. We now notice that the measurement process in noisy, each sample $X^{(n)}$ is in arbitrary scale, and thus if for two $X, \\tilde{X}$: $$ \\forall i,j: \\: X[j,i] = c\\tilde{X}[j,i], $$ for some constant $c>0$, then the two inputs are **equivalent** in their meaning.\n",
        "    * (a) For the given network, that **is already trained**, what is the effect of the scale $c$ on the classification result?\n",
        "    * (b) Can the arbitrary scale hurt the training process? Hint: think what happens to the gradient of each sample.\n",
        "    * (c) How can use this information to improve the classifier performance?\n",
        "\n",
        "\n",
        "# Answer\n",
        "* Section 1:\n",
        "\n",
        "  Looking at the definition of $f_w(X)$ it can clearly be seen that the network is equivalemt to a FC-MLP network\n",
        "$$f_w(X)=\\sum_{r=1}^{4d}W'_2[r]\\phi \\left( \\sum_{k=1}^{4d}W'_1[k,i]X'[k] \\right) $$\n",
        "  Where:\n",
        "  \n",
        "  Input layer: $4\\times d$\n",
        "  Linear layer: 4\\times d$\n",
        "  Activation layer: Leaky reLU\n",
        "  Linear layer: 1\n",
        "  Activation layer: Sign\n",
        "\n",
        "  $$X'[i]=X[mod(i,4)][i/4]$$\n",
        "  $$W'_1[r][i]=W_1[mod(r,4)][r/4][mod(i,4)][i/4]$$\n",
        "  $$W_2'[i]=W_2[mod(i,4)][i/4]$$\n",
        "  Therefore, all MLP Invariances\n",
        "  - Permutation:\n",
        "  \n",
        "  Permutation of each layer can be reversed by permuting the weights or next layer, any permutaion is possible in general, and case of permution over the ATCG dimentions\n",
        "  $$p=\\begin{bmatrix} \n",
        "  0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
        "  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n",
        "  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
        "  \\end{bmatrix}$$\n",
        "  Exmaple for ACTG to CAGT where d=3\n",
        "\n",
        "  - Re-scale:\n",
        "  For Leaky ReLU we have re-scaling simtry for c>0\n",
        "  or for c< 0 we can adjust the leaky parameter to $c\\rho$ and deviding by $\\frac{1}{c}$ after the activation layer\n",
        "* Section 2:\n",
        "\n",
        "  With the ambivalance torwards the order of the input the network needs to adhere\n",
        "  $$f_w(X)=f_w(\\tilde{X})$$\n",
        "  $$\\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]X[j, i] \\right)=$$$$=\\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]X[j,d-i+1] \\right)$$\n",
        "  Removing all common factors\n",
        "  $$\\sum_{i=1}^d W_1[r, k,j, i]X[j, i]=\\sum_{i=1}^d W_1[r, k,j, i]X[j,d- i+1] ~;~\\forall r,k,j$$\n",
        "  $$W_1[r,k,j,i]=W_1[r,k,j,d-i+1]~;~\\forall r,k,j$$\n",
        "  The weights of $W_1$ have to have a symmetry to improve the classification\n",
        "\n",
        "\n",
        "* Section 3:\n",
        "\n",
        "  Same as before, plugging the 2 inputs\n",
        "$$\\sum_{i=1}^d W_1[r, k,j, i]X[j, i]=\\sum_{i=1}^d W_1[r, k,j, i]\\tilde{X}[\\bar{j},i] ~;~\\forall r,k,j ~;~s.t~~\\{ \\bar{j}=(j+2)mod(4)\\}$$\n",
        "$$\\sum_{i=1}^d W_1[r, k,j, i]X[j, i]=\\sum_{i=1}^d W_1[r, k,j, i]X[\\bar{j},d-i+1] ~;~\\forall r,k,j$$\n",
        "$$W_1[r,k,j,i]=W_1[r,k,(j+2)mod_4,d-i+1]$$\n",
        "This constraint will cause another factor 2 decrease in number of parameters in the first layer\n",
        "* Section 4:\n",
        "  a. For a trained network, taking both inputs\n",
        "  $$f_w(X)=f_w(\\tilde{X})=\\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]\\tilde{X}[j, i] \\right)=$$\n",
        "  $$=\\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]c^{-1}X[j, i] \\right)=$$\n",
        "  $$=\\frac{1}{c}\\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]\\tilde{X}[j, i] \\right)=\\frac{1}{c}f_w(X)$$\n",
        "  The output is sign(.) of the output\n",
        "  $$\\text{sign}\\left(\\frac{1}{c}f_w(X)\\right)\\underbrace{=}_{c>0}\\text{sign}(f_w(X))$$\n",
        "  Therefore it will not change the results of the network\n",
        "\n",
        "  b. For c>0 small enough it can cause either exploding/deminishing gradients. the constant c can reach power of 3 in a gradient decent process.\n",
        "\n",
        "  c. WE can't use the same tactic as before, we will get the term $cW_1=W_1$ and it will cause $W_1=0$ So we can't have any constraints, but it can decrease model overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNB6kkMG4D2O"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 6 -VGG Architecture\n",
        "---\n",
        "\n",
        "1. The VGG-11 CNN architecture consists of 11 convolution (CONV)/fully-connected (FC) layers (every CONV layer has the same padding and stride, every MAXPOOL layer is 2×2 and has padding of 0 and stride 2). Fill in the table. You need to **consider the bias**.\n",
        "\n",
        "\n",
        "* CONV$M$-$N$: a convolutional layer with $N$ neurons, each of size $M \\times M \\times D$, where $D$ is the number of filters. $stride=1, padding=1$ \n",
        "* POOL2: $2 \\times 2$ Max Pooling with $stride=2$\n",
        "    * In case the input of the layer is odd, you should round down. For example, if the output of the layer should be $3.5 \\times 3.5 \\times 3$, you should round to $3 \\times 3 \\times 3$ (i.e., ignore the last column of the input image when performing MaxPooling).\n",
        "* FC-N: a fully connected layer with $N$ neurons.\n",
        "\n",
        "\n",
        "| Layer  | Output Dimension  | Number of Parameters (Weights) |\n",
        "|---|---|---|\n",
        "| INPUT     |  224x224x3  | 0  |\n",
        "|  CONV3-64 | 224x224x64  | 3x3x3x64  | \n",
        "| ReLU      | 224x224x64  | 0  |\n",
        "| POOL2     | 112x112x64  | 0  |\n",
        "|CONV3-128  | 112x112x128 | 3x3x64x128 |\n",
        "|ReLU       | 112x112x128 | 0 |\n",
        "| POOL2     | 66x66x128   | 0 |\n",
        "|CONV3-256  | 66x66x256   | 3x3x128x256|\n",
        "|ReLU       | 66x66x256   | 0 |\n",
        "|CONV3-256  | 66x66x256   | 3x3x256x256|\n",
        "|ReLU       | 66x66x256   | 0 |\n",
        "| POOL2     | 33x33x256   | 0 |\n",
        "|CONV3-512  | 33x33x512   | 3x3x256x512|\n",
        "|ReLU       | 33x33x512   | 0 |\n",
        "|CONV3-512  | 33x33x512   | 3x3x512x512 |\n",
        "|ReLU       | 33x33x512   | 0 |\n",
        "| POOL2     | 16x16x512   | 0 |\n",
        "|CONV3-512  | 16x16x512   | 3x3x512x512|\n",
        "|ReLU       | 16x16x512   | 0 |\n",
        "|CONV3-512  | 16x16x512   | 3x3x512x512 |\n",
        "|ReLU       | 16x16x512   | 0 |\n",
        "| POOL2     | 8x8x512     | 0  |\n",
        "| FC-4096   | 4096        | 8x8x512x4096+4096  |\n",
        "| FC-4096   | 4096        | 4096x4096+4096  |\n",
        "| FC-1000   | 1000        | 4096x1000+1000  |\n",
        "| SOFTMAX   | 1000        | 0  |\n",
        "\n",
        "2. What is the total number of parameters? (use a calculator for this one)\n",
        "    \n",
        "    Params = 9,217,728 + 155,100,136 = 164,317,864\n",
        "3. What percentage of the weights are found in the fully-connected layers?\n",
        "\n",
        "    CONV_percent = CONV_param / param = $100\\cdot \\frac{9217728}{164317864}=5.61$% "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D-14iM7pGhm"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/officel/80/000000/code.png\" style=\"height:50px;display:inline\"> Part 2 - Code Assignments\n",
        "---\n",
        "* You must write your code in this notebook and save it with the output of all of the code cells.\n",
        "* Additional text can be added in Markdown cells.\n",
        "* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUt4g58r4D2Q"
      },
      "source": [
        "#### Tips\n",
        "---\n",
        "1. Uniformly distributed tensors - `torch.Tensor(dim1, dim2, ...,dimN).uniform_(-1, 1)`\n",
        "2. Separation to **validation set** in PyTorch - <a href=\"https://gist.github.com/MattKleinsmith/5226a94bad5dd12ed0b871aed98cb123\">See example here</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oseyL0pa4D2Q",
        "outputId": "d3de5e98-ab9d-40c1-f07d-a04dd1ca6cc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f070e72f890>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# imports for the practice (you can add more if you need)\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm9qGbJw4D2S"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 - The Importance of Activation and Initialization\n",
        "---\n",
        "In this task, we are going to use $x \\in \\mathcal{R}^{512}$ and simple neural network that outputs $f(x) \\in \\mathcal{R}^{512}$. The network will have 100 layers with 512 units in each layer.\n",
        "\n",
        "1. We initialize the weights from a unit normal distribution. Run the following code cell and explain what happens. Add a short piece of code that locates when it happens (hint: use `torch.isnan()`). **Print** the layer number.\n",
        "2. We can demonstrate that at a given layer, the matrix product of inputs $x$ and weight matrix $a$ that is initialized from a standard normal distribution will, on average, have a standard deviation very close to the square root of the number of input connections. For our example, with 512 dimensions, show that for 10,000 multiplications of $a$ and $x$, the empirical standard deviation is similar to the square root of the number of input connections. Use the unbiased version: $$ \\hat{std} = \\sqrt{\\frac{\\sum_{i=1}^{10000}\\frac{1}{N}\\sum_{j=1}^N y^2}{10000}}, $$ where $y=ax$ and $N$ is the number of input connections. **Print** the mean, std and the square root of the number of input connections.\n",
        "3. For the code from 1, normalize the weight initialization by the square root of the input connections. How does that change the outcome? **Print** the mean and std after the modification.\n",
        "4. Add a `tanh()` activation after each layer for the code from 1. **Print** the mean and std after the modification. Explain the result.\n",
        "5. Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between $$\\pm \\sqrt{\\frac{6}{n_i + n_{i+1}}}$$ where $n_i$ is the number of incoming network connections, or “fan-in,” to the layer, and $ n_{i+1}$ is the number of outgoing network connections from that layer, also known as the “fan-out”. Glorot and Bengio believed that Xavier weight initialization would maintain the variance of activations and back-propagated gradients all the way up or down the layers of a network and demonstrated that networks initialized with Xavier achieved substantially quicker convergence and higher accuracy. Implement **Xavier Uniform** as `xavier_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Xavier Uniform**. Use it on the simple network from 1 with `tanh` activation. **Print** the mean and std after the modification.\n",
        "6. If you try to replace the `tanh` activation with `relu` activation in section 5, you will see very different results. Xavier strives to acheive activation outputs of each layer to have a mean of 0 and a standard deviation around 1, on average. When using a ReLU activation, a single layer will, on average have standard deviation that’s very close to the square root of the number of input connections, **divided by the square root of two** ($\\sqrt{\\frac{512}{2}}$ in our example). **Kaiming He et. al.** proposed an initialization scheme that’s tailored for deep neural nets that use these kinds of asymmetric, non-linear activations. Implement **Kaiming Normal** as `kaiming_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Kaiming Normal** (use `fan_in` mode). Use it on the simple network from 1 with `relu` activation. **Print** the mean and std after the modification. What happens when you use Xavier with RelU activation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3D8bLB24D2U",
        "outputId": "60e87d45-c79b-4c6f-ce73-2d39d908d3f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector has a NaN value at layer 28\n",
            "tensor(nan) tensor(nan)\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(512)\n",
        "for i in range(100):\n",
        "    a = torch.randn(512, 512)\n",
        "    x = a @ x\n",
        "    if torch.any(torch.isnan(x)):\n",
        "      print(r'Vector has a NaN value at layer {}'.format(i))\n",
        "      break\n",
        "print(x.mean(), x.std())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIxmCzYO4D2V"
      },
      "source": [
        "Your answers here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHVihOxp4D2V",
        "outputId": "60e38e94-bc18-4ee0-aba0-87a420a94e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empirical mean on vector y is: 0.00717\n",
            "Empirical std on vector y is: 22.62178\n",
            "Square root of input connections: 22.62742\n"
          ]
        }
      ],
      "source": [
        "# Section 2\n",
        "std = torch.Tensor([0])\n",
        "mean = torch.Tensor([0])\n",
        "itr = 10000\n",
        "in_layer = 512\n",
        "for i in range(itr):\n",
        "  x = torch.randn(in_layer)\n",
        "  a = torch.randn(in_layer, in_layer)\n",
        "  y = a @ x\n",
        "  mean += y.mean() / itr\n",
        "  std += (y @ y) / (in_layer * itr)\n",
        "std = torch.sqrt(std)\n",
        "\n",
        "print(r'Empirical mean on vector y is: {:.5f}'.format(float(mean)))\n",
        "print(r'Empirical std on vector y is: {:.5f}'.format(float(std)))\n",
        "print(r'Square root of input connections: {:.5f}'.format(float((np.sqrt(in_layer)))))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_net(in_layer=512, iter=100, std=1, dist=torch.randn, activation=torch.Tensor):\n",
        "  x = torch.randn(in_layer)\n",
        "  for i in range(iter):\n",
        "      a = dist(in_layer, in_layer) * std\n",
        "      x = activation(a @ x)\n",
        "      if torch.any(torch.isnan(x)):\n",
        "        print(r'Vector has a NaN value at layer {}'.format(i))\n",
        "        break\n",
        "  print(r'Empirical mean on x: {:05f}'.format(float(x.mean())))\n",
        "  print(r'Empirical std on x: {:05f}'.format(float(x.std())))\n",
        "\n",
        "\n",
        "# Section 3\n",
        "print(\"Normalized std\")\n",
        "simple_net(std=torch.sqrt(torch.Tensor([1 / 512])))\n",
        "\n",
        "# Section 4\n",
        "print(\"\\ntanh activation\")\n",
        "simple_net(activation=torch.tanh)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJbYnGpAJFgs",
        "outputId": "ee1abee7-0ec6-48f3-b637-15fa39841ecf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized std\n",
            "Empirical mean on x: 0.024521\n",
            "Empirical std on x: 0.873145\n",
            "\n",
            "tanh activation\n",
            "Empirical mean on x: 0.039061\n",
            "Empirical std on x: 0.980778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_init(fan_in, fan_out):\n",
        "  limit = torch.sqrt(torch.Tensor([6 / (fan_in + fan_out)]))\n",
        "  return limit * torch.Tensor(fan_in, fan_out).uniform_(-1,1)\n",
        "\n",
        "\n",
        "# Section 5\n",
        "print(\"Xavier init\")\n",
        "simple_net(dist=xavier_init, activation=torch.tanh)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py14iP303k8G",
        "outputId": "5171c7c1-cb4e-4914-a341-e9f04a648fbe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xavier init\n",
            "Empirical mean on x: 0.001691\n",
            "Empirical std on x: 0.063360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def kaiming_init(fan_in, fan_out):\n",
        "  std = torch.sqrt(torch.Tensor([2 / fan_in]))\n",
        "  return std * torch.randn(fan_in, fan_out)\n",
        "\n",
        "\n",
        "# Section 6\n",
        "print(\"Kaiming init\")\n",
        "simple_net(dist=kaiming_init, activation=torch.relu)\n",
        "\n",
        "print(\"\\nXavier init and reLU\")\n",
        "simple_net(dist=xavier_init, activation=torch.relu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e4-rOTE5c6t",
        "outputId": "9fef1c6f-21f6-440f-e0dd-119f8582333a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaiming init\n",
            "Empirical mean on x: 0.132280\n",
            "Empirical std on x: 0.189079\n",
            "\n",
            "Xavier init and reLU\n",
            "Empirical mean on x: 0.000000\n",
            "Empirical std on x: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAzlh8rU4D2W"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 - FashionMNIST Deep Classifer\n",
        "---\n",
        "In this task you are going to design and train your first neural network for classification.\n",
        "1. Load the FashionMNIST dataset `torchvision.datasets.FashionMNIST` and display 6 images with their labels from the dataset.\n",
        "2. Design a **MLP** to classify images from the FashionMNIST dataset. **You need to reach at least 85% accuracy on the test set, and 89% for a full grade**.\n",
        "    * You have a free choice of architecture, optimizer, learning scheduler, initialization, regularization and activations.\n",
        "    * In a Markdown block, write down the chosen architectures and all the hyper-parameters.\n",
        "    * **Plot** the loss curves (and any oter statistic you want) as a function of epochs/iterations.\n",
        "    * **Print** the test accuracy.\n",
        "3. Change the initialization of the linear layers and re-train the model. You can pick an initialization of your choosing from : https://pytorch.org/docs/stable/nn.init.html . See example below how to use. **Print** the change in accuracy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc_layer = [\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, 10)\n",
        "]"
      ],
      "metadata": {
        "id": "QeP-LeADGyR3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLP_layer = [nn.Linear(28*28, 1024),\n",
        "             nn.ReLU(inplace=True),\n",
        "             #nn.Linear(512, 1024),\n",
        "             #nn.ReLU(inplace=True),\n",
        "             #nn.Linear(1024, 1024),\n",
        "             #nn.ReLU(inplace=True),\n",
        "             nn.Linear(1024, 2048),\n",
        "             nn.ReLU(inplace=True),\n",
        "             nn.Linear(2048, 4096),\n",
        "             nn.ReLU(inplace=True),\n",
        "             #nn.Linear(1024, 1024),\n",
        "             #nn.ReLU(inplace=True)\n",
        "             #nn.Linear(4096, 4096),\n",
        "             #nn.ReLU(inplace=True)]\n",
        "]"
      ],
      "metadata": {
        "id": "MoylWxJB3GKu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer=[\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # 16x16x64\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "            # 8x8x128\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            #4x4x256"
      ],
      "metadata": {
        "id": "_1-3Sy2CGud3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_VOObnYp4D2W"
      },
      "outputs": [],
      "source": [
        "# example of weight initialization\n",
        "import torch.nn as nn\n",
        "\n",
        "def init_normal(m):\n",
        "  #std = torch.sqrt((torch.Tensor([1/m.shape[1]]))) * torch.sqrt(torch.Tensor([2]))\n",
        "  std = (m.shape[0] ** -0.5) * (2 ** 0.5)\n",
        "  nn.init.normal_(m, 0, std)\n",
        "\n",
        "\n",
        "def init_kaiming_normal(m):\n",
        "  nn.init.kaiming_normal_(m, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, mlp_param, lin_param, init=init_normal):\n",
        "        super(MyModel, self).__init__()\n",
        "        # model definitions/blocks\n",
        "        self.mlp_layer = nn.Sequential(*mlp_param)\n",
        "        self.fc_layer = nn.Sequential(*lin_param)\n",
        "        self.init_model = init\n",
        "        # custom initialization\n",
        "        #self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # pick initialzation: https://pytorch.org/docs/stable/nn.init.html\n",
        "                # examples\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu', a=math.sqrt(5))\n",
        "                #nn.init.normal_(m.weight, 0, 0.005)\n",
        "                # don't forget the bias term (m.bias)\n",
        "                self.init_model(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.mlp_layer(x)\n",
        "        x = self.fc_layer(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_examples(dataSet, classes,qt):\n",
        "  trainloader = torch.utils.data.DataLoader(dataSet, batch_size=qt, shuffle=True)\n",
        "  imgs, labels = iter(trainloader).next()\n",
        "  imgs = imgs / 2 + 0.5\n",
        "  imgs = imgs.numpy() #BxHxW to BxHxW\n",
        "  if (imgs.shape[1] == 3):\n",
        "    imgs = imgs.transpose(0,2,3,1)\n",
        "    cmap=\"viridis\"\n",
        "  elif (imgs.shape[1] == 1):\n",
        "    imgs = imgs.transpose(1,0,2,3)[0]\n",
        "    cmap = \"gray\"\n",
        "  print(imgs.shape) # For debug\n",
        "  rows = int((qt / 3)+0.7)\n",
        "  fig, ax = plt.subplots(3, rows, figsize=(12,12))\n",
        "  for i, img in enumerate(imgs):\n",
        "    ax[i%3][int(i/3)].imshow(img, cmap=cmap)\n",
        "    ax[i%3][int(i/3)].set_title(classes[labels[i]])\n",
        "    ax[i%3][int(i/3)].set_xticks([])\n",
        "    ax[i%3][int(i/3)].set_yticks([])"
      ],
      "metadata": {
        "id": "X-8r_MSHm_gZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "-9PeQdrw4D2W",
        "outputId": "4a06e99a-8f04-4eab-d5f8-a46bd70d9561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 28, 28)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x864 with 6 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Get Dataset and show\n",
        "train_transform = transforms.Compose([\n",
        "                                      #transforms.RandomCrop(28, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor()])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                                      transforms.ToTensor()])\n",
        "\n",
        "FMNIST_classes = ['Top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankel boot']\n",
        "train_set = torchvision.datasets.FashionMNIST(root='./datasets', train=True, download=True, transform=train_transform)\n",
        "test_set = torchvision.datasets.FashionMNIST(root='./datasets', train=False, download=True, transform=test_transform)\n",
        "\n",
        "show_examples(test_set, FMNIST_classes, 6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP dict\n",
        "# time to train our model\n",
        "# hyper-parameters\n",
        "\n",
        "batch_size = 256\n",
        "learning_rate = 1e-4\n",
        "epochs = 40\n",
        "\n",
        "\n",
        "# dataloaders - creating batches and shuffling the data\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# device - cpu or gpu?\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# loss criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# build our model and send it to the device\n",
        "model = MyModel(MLP_layer, fc_layer).to(device) # no need for parameters as we alredy defined them in the class\n",
        "\n",
        "# optimizer - SGD, Adam, RMSProp...\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10,30], gamma=0.2)\n",
        "model_dict = {'model': model,\n",
        "              'criterion': criterion,\n",
        "              'optimizer': optimizer,\n",
        "              'scheduler': scheduler,\n",
        "              'epochs': epochs,\n",
        "              'trainloader': trainloader,\n",
        "              'testloader': testloader,\n",
        "              'device': device}"
      ],
      "metadata": {
        "id": "nx948m7LC4L-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, dataloader, criterion, device):\n",
        "    model.eval() # put in evaluation mode\n",
        "    total_correct = 0\n",
        "    total_images = 0\n",
        "    total_loss = 0\n",
        "    confusion_matrix = np.zeros([10,10], int)\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_images += labels.size(0)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_loss += criterion(outputs, labels).data.item() / len(dataloader)\n",
        "            for i, l in enumerate(labels):\n",
        "                confusion_matrix[l.item(), predicted[i].item()] += 1 \n",
        "\n",
        "    model_accuracy = total_correct / total_images * 100\n",
        "    return model_accuracy, confusion_matrix, total_loss"
      ],
      "metadata": {
        "id": "W96HIA0bHO4z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model_dict):\n",
        "  model = model_dict['model']\n",
        "  criterion = model_dict['criterion']\n",
        "  optimizer = model_dict['optimizer']\n",
        "  epochs = model_dict['epochs']\n",
        "  trainloader = model_dict['trainloader']\n",
        "  testloader = model_dict['testloader']\n",
        "  device= model_dict['device']\n",
        "  scheduler = model_dict['scheduler']\n",
        "\n",
        "  # training loop\n",
        "  train_error = []\n",
        "  test_error = []\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      model.train()  # put in training mode\n",
        "      running_loss = 0.0\n",
        "      epoch_time = time.time()\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs\n",
        "          inputs, labels = data\n",
        "          # send them to device\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model(inputs)  # forward pass\n",
        "          loss = criterion(outputs, labels)  # calculate the loss\n",
        "          # always the same 3 steps\n",
        "          optimizer.zero_grad()  # zero the parameter gradients\n",
        "          loss.backward()  # backpropagation\n",
        "          optimizer.step()  # update parameters\n",
        "\n",
        "          # print statistics\n",
        "          train_error.append(loss.data.item())\n",
        "          running_loss += loss.data.item()\n",
        "\n",
        "      # Normalizing the loss by the total number of train batches\n",
        "      scheduler.step()\n",
        "      running_loss /= len(trainloader)\n",
        "\n",
        "      # Calculate training/test set accuracy of the existing model\n",
        "      train_accuracy, _, _= calculate_accuracy(model, trainloader, criterion, device)\n",
        "      test_accuracy, _, test_loss = calculate_accuracy(model, testloader, criterion, device)\n",
        "      test_error.append(test_loss)\n",
        "      log = \"Epoch: {:2d} | Loss: {:.4f} | Training accuracy: {:.3f}% | Test accuracy: {:.3f}% | \".format(epoch, running_loss, train_accuracy, test_accuracy)\n",
        "      #train_error.append(running_loss)\n",
        "      epoch_time = time.time() - epoch_time\n",
        "      log += \"Epoch Time: {:.2f} secs\".format(epoch_time)\n",
        "      print(log)\n",
        "    \n",
        "#     # save model\n",
        "#      if epoch % 20 == 0:\n",
        "#          print('==> Saving model ...')\n",
        "#          state = {\n",
        "#              'net': model.state_dict(),\n",
        "#              'epoch': epoch,\n",
        "#         }\n",
        "#          if not os.path.isdir('checkpoints'):\n",
        "#              os.mkdir('checkpoints')\n",
        "#          torch.save(state, './checkpoints/cifar_cnn_ckpt.pth')\n",
        "\n",
        "  print('==> Finished Training ...')\n",
        "\n",
        "  #fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
        "  batches = len(trainloader)\n",
        "  itr = list(range(epochs * batches))\n",
        "  plt.plot(itr, train_error, 'r--', label='Train batch loss', linewidth=0.5)\n",
        "  itr = list(range(batches,(epochs * len(trainloader)+batches), batches))\n",
        "  plt.plot(itr, test_error, color='g', label='Test Epoch loss', linewidth= 3)\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Batch')\n",
        "  plt.ylim(0, max(train_error))\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "o3N0Ho53HYqI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcjum5A5dlhi",
        "outputId": "c2271b2c-7061-4e71-b9f6-03b40e88de78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1 | Loss: 1.1250 | Training accuracy: 70.187% | Test accuracy: 69.810% | Epoch Time: 186.79 secs\n",
            "Epoch:  2 | Loss: 0.6715 | Training accuracy: 79.553% | Test accuracy: 78.700% | Epoch Time: 190.56 secs\n",
            "Epoch:  3 | Loss: 0.5526 | Training accuracy: 81.778% | Test accuracy: 80.900% | Epoch Time: 209.53 secs\n",
            "Epoch:  4 | Loss: 0.4884 | Training accuracy: 84.705% | Test accuracy: 83.180% | Epoch Time: 208.61 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(MLP_layer, fc_layer).to(device) # no need for parameters as we alredy defined them in the class\n",
        "\n",
        "# optimizer - SGD, Adam, RMSProp...\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10,30], gamma=0.2)\n",
        "\n",
        "model_dict['model'] = model\n",
        "model_dict['optimizer'] = optimizer\n",
        "model_dict['scheduler'] = scheduler\n",
        "train_model(model_dict)"
      ],
      "metadata": {
        "id": "uUqx5SvkraAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "457Q4JbR4D2X"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3 - Design a CNN\n",
        "---\n",
        "In this task you are going to design a deep convolutional neural network to classify house number digits from the **The Street View House Numbers (SVHN)** Dataset. \n",
        "\n",
        "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.\n",
        "\n",
        "* 10 classes, 1 for each digit. Digit '0' has label 0, '1' has label 1,...\n",
        "* 73257 digits for training, 26032 digits for testing, and 531131 additional, somewhat less difficult samples, to use as extra training data.\n",
        "\n",
        "<img src=\"http://ufldl.stanford.edu/housenumbers/32x32eg.png\" style=\"height:250px\">\n",
        "\n",
        "1. Load the SVHN dataset with PyTorch using `torchvision.datasets.SVHN(root, split='train', transform=None, target_transform=None, download=True)`, you can read more here: https://pytorch.org/docs/stable/torchvision/datasets.html#svhn. Display 5 images from the train set.\n",
        "2. Design a Convolutional Neural Network (CNN) to classify digits from the images.\n",
        "    * Describe the chosen architecture, how many layers? What activations did you choose? What are the filter sizes? Did you use fully-connected layers (if you did, explain their sizes)?\n",
        "    * What is the input dimension? What is the output dimension?\n",
        "    * Calculate the number of parameters (weights) in the network. **Print** this number.\n",
        "3. Train the classifier (preferably on a GPU - use Colab for this part if you don't have a GPU).\n",
        "    * Describe the the hyper-parameters of the model (batch size, epochs, learning rate....). How did you tune your model? Did you use a validation set to tune the model?\n",
        "    * What is the final accuracy on the test set? **Print** it.\n",
        "        * You need to reach at least 86% accuracy in this section, and 90% for a full grade.\n",
        "    * **Plot** the loss curves (and any other statistic you want) as a function of epochs/iterations.\n",
        "4. For the trained classifier, what is the accuracy on the test set when each test image is added a small noise $a=(0.05, 0.01, 0.005)$: $$ \\text{image} + a \\times \\mathcal{N}(0, 1) $$. **Print** the result for each value of $a$.\n",
        "5. Retrain the classifier, but this time use data augementation of your choosing. Briefly explain what augmentation you chose and how it works. Did the test accuracy improve? **Print** the result.\n",
        "    * You can use transformations available in `torchvision.transforms` as shown in the tutorial.\n",
        "    * You are welcome to use <a href=\"https://kornia.github.io/\">`kornia`</a> for the augmentations.\n",
        "    * **Plot** the loss curves (and any other statistic you want) as a function of epochs/iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au1PU2314D2X"
      },
      "outputs": [],
      "source": [
        "# Get Dataset and show\n",
        "train_transform = transforms.Compose([\n",
        "                                      transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "numbers_classes = [str(i) for i in range(9)]\n",
        "train_set = torchvision.datasets.SVHN(root='./datasets_svhn', split='train', download=True, transform=train_transform)\n",
        "test_set = torchvision.datasets.SVHN(root='./datasets_svhn', split='test', download=True, transform=test_transform)\n",
        "\n",
        "show_examples(test_set, numbers_classes, 5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, conv_param, lin_param):\n",
        "        super(CNNModel, self).__init__()\n",
        "        # model definitions/blocks\n",
        "        self.conv_layer = nn.Sequential(*conv_param)\n",
        "        self.fc_layer = nn.Sequential(*lin_param)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "GF_VpLEt5O2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN dict\n",
        "# time to train our model\n",
        "# hyper-parameters\n",
        "batch_size = 256\n",
        "learning_rate = 1e-5\n",
        "epochs = 40\n",
        "\n",
        "\n",
        "# dataloaders - creating batches and shuffling the data\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# device - cpu or gpu?\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# loss criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# build our model and send it to the device\n",
        "model = CNNModel(conv_layer, fc_layer).to(device) # no need for parameters as we alredy defined them in the class\n",
        "\n",
        "# optimizer - SGD, Adam, RMSProp...\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10,30], gamma=0.2)\n",
        "\n",
        "\n",
        "model_dict = {'model': model,\n",
        "              'criterion': criterion,\n",
        "              'optimizer': optimizer,\n",
        "              'scheduler': scheduler,\n",
        "              'epochs': epochs,\n",
        "              'trainloader': trainloader,\n",
        "              'testloader': testloader,\n",
        "              'device': device}"
      ],
      "metadata": {
        "id": "tcZ0UvAy4_RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model_dict)"
      ],
      "metadata": {
        "id": "welMMDt-56JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class additive_noise():\n",
        "  def __init__(self, a):\n",
        "    self.gain = torch.Tensor([a])\n",
        "\n",
        "  def forward(imgs):  \n",
        "    shape = imgs.shape\n",
        "    WN = self.gain * torch.randn(imgs.shape)\n",
        "    imgs = imgs + WN\n",
        "    return torch.clamp(imgs, min=-1, max=1)"
      ],
      "metadata": {
        "id": "2BwghJ9t3v2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gains = [0.005, 0.01, 0.05]\n",
        "for gain in gains:\n",
        "  test_transform = transforms.Compose([\n",
        "                                     transforms.ToTensor(),\n",
        "                                     #additive_noise(gain),\n",
        "                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "  ])\n",
        "\n",
        "  test_set = torchvision.datasets.SVHN(root='./datasets_svhn', split='test', download=True, transform=test_transform)\n",
        "  testloader = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  test_accuracy, _, _ = calculate_accuracy(model_dict['model'], testloader, criterion, device)\n",
        "  log = \"With noise with gain: {:.3f} Test accuracy: {:.3f}%\".format(gain, test_accuracy)\n",
        "  print(log)"
      ],
      "metadata": {
        "id": "jFigBWQC2wrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-rNik5j4D2X"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
        "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
        "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "60rUiyA8JP3E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "ee046211_hw2_312775364.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}